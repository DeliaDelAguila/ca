---
title: "CA - S7: Association Analysis"
author: Josep Curto, IE Business School
abstract: "This document introduces how to calculate Association Analysis with R"
keywords: "r, association analysis"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  html_notebook: 
    fig_caption: yes
    toc: yes
    toc_float: yes
    self_contained: yes
---

# Calculate Association Analysis with R

## Load packages

```{r packages, warning=FALSE, echo=FALSE, message=FALSE}
# Cleaning the environment
rm(list=ls())

# List of packages for session
.packages <- c("arules","arulesViz")

# Install CRAN packages (if not already installed)
.inst <- .packages %in% installed.packages()
if(length(.packages[!.inst]) > 0) install.packages(.packages[!.inst])

# Load packages into session 
suppressPackageStartupMessages(invisible(lapply(.packages, library, character.only = TRUE)))
```

## Loading data

First we load the data:

```{r}
edata <- readRDS("data/transactions.RDs")
```

The summary give us information about the attributes:

```{r}
summary(edata)
```

## Data preparation (I)

Our data represents the transaction ID and the product description. We must transform the data type:

```{r}
edata$id <- factor(edata$id)
edata$item <- factor(edata$item)
```

## Exploratory Data Analysis (I)

After the initial data preparation, we can observe:

```{r}
summary(edata)
```

We can try to answer few questions:

**How many transactions do we have**

```{r}
length(unique(edata$id))
```

**How many products per transaction (in average)**

```{r}
nrow(edata)/length(unique(edata$id))
```

**How popular is each one of the products**

```{r}
sapply(split(edata$item,edata$item),length)
```

**Question: What we can observe?**

## Data Preparation (II)

We can prepare further our data creating transactions:

```{r}
# Prepare data
i <- split (edata$item, edata$id)

# Transform into transaction object
txn <- as(i,"transactions")
txn
```

## Exploratory Data Analysis (II)

Let's explore this new object:

```{r}
summary(txn)
```

We can review the content:

```{r}
inspect(head(txn, n=15))
```

We can inspect binary incidence matrices

```{r}
image(txn)
```

We can obtain the popularity of the products:

```{r}
# Product frequency
itemFrequency(txn)
```

```{r}
# Plot the frequency of items sets
itemFrequencyPlot(txn)
```

And the popularity of the transactions:

```{r}
frequentItems <- eclat(txn, parameter = list(supp = 0.01, maxlen = 15))
```

```{r}
inspect(frequentItems)
```

Plot frequent items:

```{r}
itemFrequencyPlot(txn, topN=10, type="absolute", main="Item Frequency")
```

## Understanding product correlation

```{r}
# Similarity between items
d <- dissimilarity(txn, method = "phi", which = "items")
d[is.na(d)] <- 1 # get rid of missing values
plot(hclust(d), cex=.5)
```

# Association Analysis

##  Calculate Association Analysis

First we apply the algorithm with two low values for support and confidence (as we want to obtain as many rules as possible):

```{r applying apropri algorithm}
basket_rules <- apriori(txn,parameter = list(sup = 0.005, conf=0.001, minlen=1, target = "rules"))
```

**Question: what happens if you change minlen by 1?**

Then we understand the output:

```{r understanding the output}
summary(basket_rules)
```

We can review the result (and order by lift)

```{r inspect rule}
inspect(basket_rules, by = "lift", decreasing=TRUE, n=15)
```

We can find the significant rules:

```{r}
# Find rules where the LHS and the RHS depend on each other.
inspect(basket_rules[is.significant(basket_rules, txn)])
```

Some itemsets are redundant because they have identical support as their supersets. We can find the redundant rules:

```{r redundant rules}
inspect(basket_rules[is.redundant(basket_rules)])
```

We can find the non-redundant rules:

```{r find non redundant rules}
inspect(basket_rules[!is.redundant(basket_rules)])
```

We can measure more interesting measure if it is required:

```{r}
# We can create a dataframe to save all the metrics and analyze them in detail
df_rules <- interestMeasure(basket_rules, c("support", "chiSquare", "confidence", "conviction",
                                            "cosine", "coverage", "leverage", "lift", "oddsRatio"), txn)
df_rules
```

## Visual Analytics

**Scatter Plot**

```{r plot 1}
# Plotting the output
plot(basket_rules)
```

**Graph**

```{r plot 2}
# Another (better) visualization
plot(basket_rules,
     method="graph",
     measure="confidence",
     shading="lift", control=list(type="items"))
```

**Question: What is happening?**

## Refining our analysis

We apply the algorithm with new values:

```{r}
# Refining our analysis
basket_rules.refined <- apriori(txn,parameter = list(minlen=2, sup = 0.05, conf = 0.2,target="rules"))
```

The summary of the result:

```{r}
summary(basket_rules.refined)
```

Let's obtain the top 10 rules by lift:

```{r}
inspect(head(basket_rules.refined, n=10, by = "lift"))
```

An itemset is maximal frequent if none of its immediate supersets is frequent. We can find the maximal rules:

```{r}
# Finding the maximal
maximal <- is.maximal(basket_rules.refined)
inspect(basket_rules.refined[maximal])
```

Subset based on an item

```{r}
inspect(subset(basket_rules.refined, subset = items %in% "yogurt"))
```

## More visual Analytics

**Scatter Plot**

```{r}
# Plotting the output
plot(basket_rules.refined)
```

**Graph**

```{r}
plot(basket_rules.refined,
     method="graph",
     measure="confidence",
     shading="lift", control=list(type="items"))
```

**Grouped Graph**

```{r}
# Another way
plot(basket_rules.refined, method = "grouped")
```

**Question: what is happening?**
**Question: what do you recommend?**