---
title: "CA - S7: Example"
author: Josep Curto, IE Business School
abstract: "This document introduces how to calculate Association Analysis with R"
keywords: "r, association analysis"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  html_notebook: 
    fig_caption: yes
    toc: yes
    toc_float: yes
    self_contained: yes
---

# Calculate Association Analysis with R

## Load packages

```{r packages, warning=FALSE, echo=FALSE, message=FALSE}
# Cleaning the environment
rm(list=ls())

# List of packages for session
.packages = c("fpc","arules", "arulesViz")

# Install CRAN packages (if not already installed)
.inst <- .packages %in% installed.packages()
if(length(.packages[!.inst]) > 0) install.packages(.packages[!.inst])

# Load packages into session 
suppressPackageStartupMessages(library(arules))
suppressPackageStartupMessages(library(arulesViz))
```

## Loading data

First we load the data:

```{r load data}
# Load data
edata <- read.csv("data/s7.csv",stringsAsFactors=FALSE, sep=',')
```

The summary give is information about the attributes:

```{r summary}
summary(edata)
```

It is needed to improve the data:

```{r prepare data}
# data preparation
edata$id_purchase <- factor(edata$id_purchase)
edata$product <- factor(edata$product)
```

We can try to answer few questions:

```{r improve data}
# Some insights
length(unique(edata$id_purchase))
nrow(edata)/length(unique(edata$id_purchase))
sapply(split(edata$product,edata$product),length)
pList<-t(as.data.frame(lapply(split(edata$product,edata$product),length)))
head(pList[order(pList[,1], decreasing= T),], n = 1)
tail(pList[order(pList[,1], decreasing= T),], n = 1)

# Review data
dim(edata)
summary(edata)
```

**Question: What we can observe?**

# Data Preparation

## Creating transactions

```{r}
# Prepare data
i <- split (edata$product, edata$id_purchase)

# Transform into transaction object
txn <- as(i,"transactions")
txn

# Explore transactions
summary(txn)
inspect(txn)

# Inspect binary incidence matrices
image(txn)

# Product frequency
itemFrequency(txn)

# Plot the frequency of items sets
itemFrequencyPlot(txn)
```

## Understanding product correlation

```{r}
# Similarity between items
d <- dissimilarity(txn, method = "phi", which = "items")
d[is.na(d)] <- 1 # get rid of missing values
plot(hclust(d), cex=.5)
```

# Association Analysis

##  Calculate Association Analysis

First we apply the algorithm with two low values for support and confidence (as we want to obtain as many rules as possible):

```{r applying apropri algorithm}
basket_rules <- apriori(txn,parameter = list(sup = 0.005, conf=0.001, minlen=1, target = "rules"))
```

**Question: what happens if you change minlen by 1?**

Then we understand the output:

```{r understanding the output}
summary(basket_rules)
```

We can review the result (and order by lift)

```{r inspect rule}
inspect(basket_rules, by = "lift")
```

We can find the significant rules:

```{r}
# Find rules where the LHS and the RHS depend on each other.
inspect(basket_rules[is.significant(basket_rules, txn)])
```

Some itemsets are redundant because they have identical support as their supersets. We can find the redundant rules:

```{r redundant rules}
inspect(basket_rules[is.redundant(basket_rules)])
```

We can find the non-redundant rules:

```{r find non redundant rules}
inspect(basket_rules[!is.redundant(basket_rules)])
```

We can measure more interesting measure if it is required:

```{r}
# We can create a dataframe to save all the metrics and analyze them in detail
df_rules <- interestMeasure(basket_rules, c("support", "chiSquare", "confidence", "conviction",
                                            "cosine", "coverage", "leverage", "lift", "oddsRatio"), txn)
df_rules
```

## Visual Analytics

**Scatter Plot**

```{r plot 1}
# Plotting the output
plot(basket_rules)
```

**Graph**

```{r plot 2}
# Another (better) visualization
plot(basket_rules,
     method="graph",
     measure="confidence",
     shading="lift", control=list(type="items"))
```

**Question: What is happening?**

## Refining our analysis

We apply the algorithm with new values:

```{r}
# Refining our analysis
basket_rules.refined <- apriori(txn,parameter = list(minlen=2, sup = 0.05, conf = 0.2,target="rules"))
```

The summary of the result:

```{r}
summary(basket_rules.refined)
```

And the rules we obtain are:

```{r}
inspect(head(basket_rules.refined, n=8, by = "lift"))
```

An itemset is maximal frequent if none of its immediate supersets is frequent. We can find the maximal rules:

```{r}
# Finding the maximal
maximal <- is.maximal(basket_rules.refined)
inspect(basket_rules.refined[maximal])
```

Subset based on an item

```{r}
inspect(subset(basket_rules.refined, subset = items %in% "A"))
```

## More visual Analytics

**Scatter Plot**

```{r}
# Plotting the output
plot(basket_rules.refined)
```

**Graph**

```{r}
plot(basket_rules.refined,
     method="graph",
     measure="confidence",
     shading="lift", control=list(type="items"))
```

**Simplified Graph**

```{r}
plot(basket_rules.refined, method = "graph",
     control = list(main = "Rules", type = "itemsets"))
```

**Grouped Graph**

```{r}
# Another way
plot(basket_rules.refined, method = "grouped")
```

**Question: what is happening?**
**Question: what do you recommend?**